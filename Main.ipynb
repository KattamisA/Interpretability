{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.optim\n",
    "from functions.models.optim import *\n",
    "\n",
    "from skimage.measure import compare_psnr\n",
    "from functions.models import *\n",
    "from copy import deepcopy\n",
    "from functions.utils.global_parameters import *\n",
    "from functions.utils.common_utils import torch_to_np\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "#dtype = torch.cuda.FloatTensor\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dip(img_np, arch = 'default', LR = 0.01, num_iter = 1000, reg_noise_std = 1.0/30,exp_weight = 0.99, INPUT = 'noise', save = False, save_path = '', plot = True, input_depth = None, name = None, loss_fn = \"MSE\", OPTIMIZER = \"adam\", pad = 'zero',  OPT_OVER = 'net' ):\n",
    "    \n",
    "    glparam = global_parameters()\n",
    "    glparam.set_params(save, plot, reg_noise_std, exp_weight)\n",
    "    glparam.load_images(img_np)\n",
    "    glparam.img_torch = glparam.img_torch.type(dtype)\n",
    "    \n",
    "    if arch == 'simple':\n",
    "        if input_depth == None:\n",
    "            input_depth = 3 \n",
    "        glparam.net = get_net(input_depth,'skip', pad,\n",
    "                skip_n33d=16, \n",
    "                skip_n33u=16, \n",
    "                skip_n11=0, \n",
    "                num_scales=3,\n",
    "                upsample_mode='bilinear').type(dtype)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    net_input = get_noise(input_depth, INPUT, (glparam.img_np.shape[1], glparam.img_np.shape[2])).type(dtype).detach()   \n",
    "    glparam.net_input_saved = net_input.detach().clone()\n",
    "    glparam.noise = net_input.detach().clone()\n",
    "    \n",
    "    # Compute number of parameters\n",
    "    param_numbers  = sum([np.prod(list(p.size())) for p in glparam.net.parameters()]) \n",
    "    print ('\\n Number of params: %d' % param_numbers)\n",
    "\n",
    "    # Loss function\n",
    "    if loss_fn == 'MSE':\n",
    "        criterion = torch.nn.MSELoss().type(dtype)\n",
    "    if loss_fn == 'KLDiv':\n",
    "        criterion = torch.nn.KLDivLoss().type(dtype)\n",
    "        \n",
    "    if save == True:\n",
    "        f= open(\"{}/Stats.txt\".format(save_path),\"w+\")\n",
    "        f.write(\"{:>11}{:>12}{:>12}\\n\".format('Iterations','Total_Loss','PSNR'))\n",
    "        save_net_details(save_path, arch, param_numbers, pad, OPT_OVER, OPTIMIZER, input_depth,\n",
    "                 loss_fn = loss_fn, LR = LR, num_iter = num_iter, exp_weight = glparam.exp,\n",
    "                 reg_noise_std = reg_noise_std, INPUT = 'INPUT', net = glparam.net)\n",
    "                \n",
    "    def closure(iter_value):\n",
    "        show_every = 100\n",
    "        figsize = 4\n",
    "        \n",
    "        ## Initialiaze/ Update variables\n",
    "        if glparam.noise_std > 0.0:\n",
    "            net_input = glparam.net_input_saved + (glparam.noise.normal_() * glparam.noise_std)\n",
    "        net_input = torch.tensor(net_input, dtype=torch.float32, requires_grad=True)\n",
    "        out = glparam.net(net_input)#, requires_grad=True)\n",
    "\n",
    "        ## Exponential Smoothing\n",
    "        if glparam.out_avg is None:\n",
    "            glparam.out_avg = out.detach()\n",
    "        else:\n",
    "            glparam.out_avg = glparam.out_avg * glparam.exp + out.detach() * (1 - glparam.exp)\n",
    "        \n",
    "        ## Calculate loss\n",
    "        total_loss = criterion(out, glparam.img_torch)\n",
    "        set_trace()\n",
    "        total_loss.backward()\n",
    "        set_trace()\n",
    "        \n",
    "        glparam.psnr_noisy = compare_psnr(glparam.img_np, out.detach().cpu().numpy()[0]).astype(np.float32)\n",
    "            \n",
    "        print ('DIP Iteration {:>11}   Loss {:>11.7f}   PSNR_noisy: {:>5.4f}'.format(\n",
    "            iter_value, total_loss.item(), glparam.psnr_noisy), end='\\r')\n",
    "        \n",
    "        ## Backtracking   \n",
    "        if (glparam.psnr_noisy_last - glparam.psnr_noisy) > 5.0:\n",
    "            glparam.interrupts = glparam.interrupts + 1\n",
    "            print('\\n Falling back to previous checkpoint.')\n",
    "            glparam.net.load_state_dict(glparam.last_net.state_dict())\n",
    "            glparam.optimizer.load_state_dict(glparam.optimizer_last.state_dict())\n",
    "            \n",
    "            if glparam.interrupts > 3:\n",
    "                glparam.psnr_noisy_last = glparam.psnr_noisy\n",
    "                \n",
    "            if OPTIMIZER == \"adam\":     \n",
    "                for j in range(iter_value % show_every - 1):                \n",
    "                    glparam.optimizer.zero_grad()\n",
    "                    closure(iter_value - (iter_value % show_every) + j + 1)\n",
    "                    glparam.optimizer.step()\n",
    "                glparam.optimizer.zero_grad()\n",
    "                closure(iter_value)          \n",
    "                print('\\n Return back to the original')                        \n",
    "                return total_loss \n",
    "            \n",
    "            if OPTIMIZER == \"EntropySGD\":\n",
    "                for j in range(iter_value % show_every - 1):\n",
    "                    glparam.optimizer.zero_grad()\n",
    "                    glparam.optimizer.step(iter_value - (iter_value % show_every) + j + 1, closure, glparam.net, criterion)\n",
    "                glparam.optimizer.zero_grad()\n",
    "                closure(iter_value)   \n",
    "                print('\\n Return back to the original')                        \n",
    "                return total_loss                      \n",
    "            \n",
    "        if (iter_value % show_every) == 0: \n",
    "            glparam.last_net = deepcopy(glparam.net)\n",
    "            glparam.psnr_noisy_last = glparam.psnr_noisy\n",
    "            glparam.optimizer_last = deepcopy(glparam.optimizer)\n",
    "            \n",
    "            if glparam.interrupts > 3 :\n",
    "                print(\"\\n Error, was not able to converge after reset\")\n",
    "            glparam.interrupts = 0\n",
    "            \n",
    "            if glparam.PLOT:\n",
    "                fig=plt.figure(figsize=(16, 16))\n",
    "                fig.add_subplot(1, 3, 1)\n",
    "                plt.imshow(np.clip(torch_to_np(out), 0, 1).transpose(1, 2, 0))\n",
    "                plt.title('Output')\n",
    "                fig.add_subplot(1, 3, 2)\n",
    "                plt.imshow(np.clip(torch_to_np(glparam.out_avg), 0, 1).transpose(1, 2, 0))\n",
    "                plt.title('Averaged Output')\n",
    "                fig.add_subplot(1, 3, 3)\n",
    "                plt.title('Original/Target')\n",
    "                plt.imshow(glparam.img_np.transpose(1, 2, 0))\n",
    "                plt.show()\n",
    "                \n",
    "            if glparam.save:\n",
    "                f = open(\"{}/Stats.txt\".format(save_path),\"a\")\n",
    "                f.write(\"{:>11}{:>12.8f}{:>12.8f}\\n\".format(iter_value, total_loss.item(), glparam.psnr_noisy))\n",
    "                plt.imsave(\"{}/it_{}.png\".format(save_path,iter_value),\n",
    "                       np.clip(torch_to_np(glparam.out_avg), 0, 1).transpose(1,2,0), format=\"png\")\n",
    "                \n",
    "        return total_loss\n",
    "        \n",
    "    ### Optimize\n",
    "    glparam.net.train()\n",
    "    p = get_params(OPT_OVER, glparam.net, net_input)\n",
    "    \n",
    "    if OPTIMIZER == \"adam\":\n",
    "        glparam.optimizer = torch.optim.Adam(p, lr = LR)\n",
    "        for j in range(num_iter):\n",
    "            glparam.optimizer.zero_grad()\n",
    "            closure(j)\n",
    "            glparam.optimizer.step()            \n",
    "    if OPTIMIZER == \"EntropySGD\":\n",
    "        glparam.optimizer = EntropySGD(p,config=dict(lr = LR))\n",
    "        for j in range(num_iter):\n",
    "            glparam.optimizer.zero_grad()\n",
    "            glparam.optimizer.step(j, closure, glparam.net, criterion)    \n",
    "    print('\\n')       \n",
    "    \n",
    "    out = glparam.net(net_input)\n",
    "    glparam.out_avg = glparam.out_avg * glparam.exp + out.detach() * (1 - glparam.exp)\n",
    "    return glparam.out_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of params: 20355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-5-3409fdc6e08d>(60)closure()\n",
      "     58         total_loss = criterion(out, glparam.img_torch)\n",
      "     59         set_trace()\n",
      "---> 60         total_loss.backward()\n",
      "     61         set_trace()\n",
      "     62 \n",
      "\n",
      "ipdb> c\n",
      "> <ipython-input-5-3409fdc6e08d>(63)closure()\n",
      "     61         set_trace()\n",
      "     62 \n",
      "---> 63         glparam.psnr_noisy = compare_psnr(glparam.img_np, out.detach().cpu().numpy()[0]).astype(np.float32)\n",
      "     64 \n",
      "     65         print ('DIP Iteration {:>11}   Loss {:>11.7f}   PSNR_noisy: {:>5.4f}'.format(\n",
      "\n",
      "ipdb> net_input.grad.data.numpy()\n",
      "array([[[[ 1.59423067e-07,  1.75658705e-08, -1.38406193e-07, ...,\n",
      "           2.26717830e-07, -2.60561023e-07,  1.25579938e-08],\n",
      "         [-7.58807559e-08, -1.14300065e-07, -1.25033779e-07, ...,\n",
      "           1.11047669e-07, -3.28132899e-09,  4.95357085e-07],\n",
      "         [ 1.25042391e-07, -3.29774025e-07,  1.30315314e-07, ...,\n",
      "           7.55471703e-08, -1.80026696e-08, -3.64283778e-07],\n",
      "         ...,\n",
      "         [-1.82531124e-07,  1.99314542e-07,  5.97264744e-08, ...,\n",
      "          -1.25090107e-08, -2.64889835e-08, -1.33196050e-08],\n",
      "         [-8.49967847e-08,  1.64481122e-07, -3.48418112e-08, ...,\n",
      "          -3.04311243e-08, -1.55428612e-08,  1.02179856e-08],\n",
      "         [ 2.96057934e-09,  6.51271890e-08, -2.79215850e-08, ...,\n",
      "           7.46710427e-09,  1.94853502e-08,  1.30299851e-08]],\n",
      "\n",
      "        [[ 4.90681913e-08,  1.85600243e-07, -9.69107390e-08, ...,\n",
      "           2.62183733e-07, -2.36103759e-08, -9.55994111e-08],\n",
      "         [ 1.32874419e-07, -2.20641581e-07,  3.20837216e-07, ...,\n",
      "          -2.56716589e-07,  4.76399777e-07, -1.17216267e-07],\n",
      "         [ 6.76239296e-08, -2.88726881e-07,  1.11091630e-07, ...,\n",
      "          -4.04538810e-07, -4.15767909e-09, -1.66101714e-07],\n",
      "         ...,\n",
      "         [-6.80469014e-10, -1.67465515e-07, -1.30361528e-07, ...,\n",
      "           3.28728653e-08, -2.43720280e-08,  7.97170152e-09],\n",
      "         [ 9.43819671e-08, -8.08204845e-08,  1.05693175e-07, ...,\n",
      "           4.41048584e-08,  8.62059224e-09, -1.23677735e-09],\n",
      "         [ 5.90172444e-09, -7.92733630e-08,  1.20586989e-07, ...,\n",
      "           1.15737491e-08, -1.28232145e-08, -1.27655406e-08]],\n",
      "\n",
      "        [[-2.59807749e-08,  1.90605022e-07, -1.17581109e-07, ...,\n",
      "          -4.12268619e-08,  6.81461820e-09, -2.56266105e-07],\n",
      "         [ 6.78926000e-08, -6.07400210e-08, -3.00080103e-08, ...,\n",
      "          -2.47375056e-07, -3.54341637e-07,  2.09270155e-07],\n",
      "         [ 1.17299273e-07,  1.57491797e-07, -1.89906132e-07, ...,\n",
      "           1.24740126e-07, -4.46649793e-08,  1.26702503e-07],\n",
      "         ...,\n",
      "         [-6.27007211e-08,  2.93930952e-07, -2.54184442e-08, ...,\n",
      "           1.18424005e-07,  2.71395049e-08, -1.00400843e-08],\n",
      "         [ 1.41142262e-08,  1.22232549e-07, -7.85507481e-09, ...,\n",
      "           2.01057979e-08,  1.40063685e-08, -1.93376639e-08],\n",
      "         [-4.43323565e-08,  1.34853781e-08, -4.42544064e-08, ...,\n",
      "           1.63416729e-08, -1.88564808e-08, -9.08724651e-09]]]],\n",
      "      dtype=float32)\n",
      "ipdb> np.average(net_input.grad.data.numpy())\n",
      "9.873149e-12\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread('data/goldfish.jpg')[..., ::-1]\n",
    "img_np = np.array(img)\n",
    "_, dip(img_np, arch = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
